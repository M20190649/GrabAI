{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, datasets and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "num_core = max(multiprocessing.cpu_count()-1,1)\n",
    "\n",
    "import gc\n",
    "\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: change const to CAPS\n",
    "from utils import prep_holdout_set, get_timespan, get_timespan_15, gen_features, gen_test_features, prep_dataset, extract_time\n",
    "from utils import time_features, num_labels, num_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: gen or parse datetime for train set\n",
    "# do same for holdout set in final notebook\n",
    "\n",
    "df = pd.read_csv(\"./data/df_transformed_seasonal_10.csv\") # \"data/df_transformed_simple_fill_noise.csv\"\n",
    "df = df.set_index('geohash6')\n",
    "df.columns = pd.DatetimeIndex(df.columns)\n",
    "\n",
    "cluster_df = pd.read_csv(\"./data/cluster_df.csv\")\n",
    "cluster_df = cluster_df.set_index('geohash6')\n",
    "\n",
    "df_raw = pd.read_csv(\"./data/datetime_coords_dow.csv\")\n",
    "df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])\n",
    "df_holdout_raw = df_raw.loc[(df_raw.datetime>pd.datetime(2019,5, 17, 12, 0)) & (df_raw.datetime<pd.datetime(2019,5, 24, 12, 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create holdout set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac to speed up, tests show not much difference in performance\n",
    "\n",
    "df_holdout_raw = df_holdout_raw.sample(frac=0.1, random_state=8) \n",
    "\n",
    "# Split into partitions for multiprocessing\n",
    "partition_size = math.floor(len(df_holdout_raw.index) / num_core)\n",
    "holdout_partitions = [df_holdout_raw.iloc[i*partition_size:i*partition_size+partition_size,:] for i in range(0, num_core)]\n",
    "\n",
    "# Add remainders after dividing Dataframe\n",
    "if (num_core * partition_size < len(df_holdout_raw.index)): \n",
    "    leftover = df_holdout_raw[num_core * partition_size:]\n",
    "    holdout_partitions[num_core-1] = pd.concat([holdout_partitions[num_core-1], leftover], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing to generate \"transformed\" holdout set, used to generate test set\n",
    "\n",
    "results = []\n",
    "if __name__ == '__main__':\n",
    "    with Pool(num_core) as p:\n",
    "        results =  p.map_async(partial(prep_holdout_set, df=df, cluster_df=cluster_df, groupby_sets={}), holdout_partitions)\n",
    "        p.close()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: change time to be part of feature generation\n",
    "\n",
    "dt_holdout = datetime(2019, 5, 20) # arbitrary, value does not matter\n",
    "df_holdout, geohash_holdout = [], []\n",
    "\n",
    "results = [i for i in results.get()]\n",
    "for i in range(len(results)):\n",
    "    df_holdout.append(results[i][0])\n",
    "    geohash_holdout.append(results[i][1])\n",
    "\n",
    "geohash_holdout = sum(geohash_holdout, [])\n",
    "df_holdout = pd.DataFrame(sum(df_holdout, []))\n",
    "df_holdout.columns = pd.date_range(dt_holdout - timedelta(minutes=15 * num_req), dt_holdout + timedelta(minutes=15 * num_labels),\n",
    "                         freq=\"15min\")\n",
    "\n",
    "df_holdout['geohash6'] = geohash_holdout\n",
    "df_holdout = df_holdout.set_index('geohash6')\n",
    "\n",
    "time_holdout = df_holdout_raw['datetime'].apply(extract_time)\n",
    "time_holdout = pd.DataFrame(time_holdout.tolist(), columns=time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 0\n",
    "model_list = []\n",
    "y_pred, labels_list = [], [] #  val\n",
    "y_pred2, labels_list2 = [], [] # test\n",
    "\n",
    "dt_train = datetime(2019,5,17, 11, 45)\n",
    "dt_val = dt_train + timedelta(days=7) # beyond 1 week\n",
    "\n",
    "dt_holdout = datetime(2019, 5, 20) # arbitrary # Make sure same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 65 # to use: 400\n",
    "\n",
    "X_train, y_train = prep_dataset(dt_train, num_samples, df, cluster_df)\n",
    "X_val, y_val = prep_dataset(dt_val, 1, df, cluster_df)\n",
    "\n",
    "X_holdout, y_holdout = gen_test_features(dt_holdout, df_holdout, geohash_holdout, cluster_df)\n",
    "\n",
    "# replace with real time features\n",
    "X_holdout = X_holdout.drop(time_features, axis=1)\n",
    "X_holdout = pd.concat([X_holdout.reset_index(),time_holdout], axis=1).set_index('geohash6')\n",
    "X_holdout = X_holdout.reindex_axis(X_train.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: pickle fitted scaler for production\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_holdout]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_holdout[:] = scaler.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 200 # to use 2000\n",
    "params = {\n",
    "    'learning_rate' : 0.1,\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 800,\n",
    "    'min_split_gain': 0.03,\n",
    "     'subsample_for_bin': 3000,\n",
    "  'bagging_fraction': 0.5 # for speed, doesnt affect score\n",
    "}\n",
    "\n",
    "cate_vars = ['demand-3500', 'demand-3000', 'demand-2800', 'demand-2500', 'demand-2000',\n",
    "            'lat', 'lon', 'grid_id_by_lat', 'grid_id_by_lon']\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "print('Model {}'.format(round))\n",
    "\n",
    "dtrain = lgb.Dataset(\n",
    "            X_train, label=y_train,\n",
    "            categorical_feature=cate_vars)\n",
    "\n",
    "dval = lgb.Dataset(\n",
    "            X_holdout, label=y_holdout, reference=dtrain,\n",
    "            categorical_feature=cate_vars)\n",
    "\n",
    "model = lgb.train(\n",
    "                params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "                valid_sets=[dtrain, dval], early_stopping_rounds=20,\n",
    "                verbose_eval=50, evals_result=evals_result)\n",
    "    \n",
    "model_list.append(model)\n",
    "# to do: save model to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle prediction\n",
    "\n",
    "print(\"Predictions for T+%s\", round+1)\n",
    "\n",
    "y_pred.append(model.predict(X_val).tolist())\n",
    "labels_list.append(y_val.tolist())\n",
    "\n",
    "y_pred2.append(model.predict(X_holdout).tolist())\n",
    "labels_list2.append(y_holdout.tolist())\n",
    "\n",
    "val_rms = math.sqrt(mean_squared_error([item for items in y_pred for item in items], [item for items in labels_list for item in items]))\n",
    "print(\"Validation rmse:\", val_rms)\n",
    "\n",
    "holdout_rms = math.sqrt(mean_squared_error([item for items in y_pred2 for item in items], [item for items in labels_list2 for item in items]))\n",
    "print(\"Holdout rmse:\", holdout_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling forecast\n",
    "# Rerun all cells starting from \"Generate train, val, test sets\" to get the next set of predictions\n",
    "# Repeat until T+5 to get the overall holdout rmse\n",
    "\n",
    "round += 1\n",
    "dt_train = dt_train + timedelta(minutes=15)\n",
    "dt_val = dt_train + timedelta(days=7)\n",
    "dt_holdout = dt_holdout + timedelta(minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: save predictions\n",
    "# save models here too if possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
