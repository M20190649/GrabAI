{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_holdout has NaN x 499\n",
    "# probably issue with filling by noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_holdout.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, datasets and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "num_core = max(multiprocessing.cpu_count()-1,1)\n",
    "\n",
    "import gc\n",
    "\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad practice, to fix in future\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import prep_holdout_set, get_timespan, get_timespan_15, gen_features, gen_test_features, prep_dataset\n",
    "from utils import TIME_FEATURES, NUM_LABELS, NUM_REQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data_temp/df_transformed_filled.csv\") # \"data/df_transformed_simple_fill_noise.csv\"\n",
    "df = df.set_index('geohash6')\n",
    "df.columns = pd.DatetimeIndex(df.columns)\n",
    "\n",
    "cluster_df = pd.read_csv(\"../data_temp/cluster_df.csv\")\n",
    "cluster_df = cluster_df.set_index('geohash6')\n",
    "\n",
    "# we use last 7 days (25/5 - 31/5) as holdout set\n",
    "# removed from train set before training\n",
    "df_raw = pd.read_csv(\"../data_temp/training_timestamps.csv\")\n",
    "df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])\n",
    "df_holdout_raw = df_raw.loc[(df_raw.datetime>pd.datetime(2019,5, 25, 0, 0)) & (df_raw.datetime<pd.datetime(2019,5, 31, 23, 45))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intermediate holdout set\n",
    "#### from which to extract holdout features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac to speed up, tests show not much difference in performance\n",
    "\n",
    "df_holdout_raw = df_holdout_raw.sample(frac=0.1, random_state=8) \n",
    "\n",
    "# Split into partitions for multiprocessing\n",
    "partition_size = math.floor(len(df_holdout_raw.index) / num_core)\n",
    "holdout_partitions = [df_holdout_raw.iloc[i*partition_size:i*partition_size+partition_size,:] for i in range(0, num_core)]\n",
    "\n",
    "# Add remainders after dividing Dataframe\n",
    "if (num_core * partition_size < len(df_holdout_raw.index)): \n",
    "    leftover = df_holdout_raw[num_core * partition_size:]\n",
    "    holdout_partitions[num_core-1] = pd.concat([holdout_partitions[num_core-1], leftover], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing to generate \"transformed\" holdout set, used to generate test set\n",
    "\n",
    "results = []\n",
    "if __name__ == '__main__':\n",
    "    with Pool(num_core) as p:\n",
    "        results =  p.map_async(partial(prep_holdout_set, df=df), holdout_partitions)\n",
    "        p.close()\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: change time to be part of feature generation\n",
    "\n",
    "dt_holdout = datetime(2019, 5, 20) # arbitrary, value does not matter\n",
    "df_holdout, geohash_holdout = [], []\n",
    "\n",
    "results = [i for i in results.get()]\n",
    "for i in range(len(results)):\n",
    "    df_holdout.append(results[i][0])\n",
    "    geohash_holdout.append(results[i][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_holdout = sum(geohash_holdout, [])\n",
    "df_holdout = pd.DataFrame(sum(df_holdout, []))\n",
    "df_holdout.columns = pd.date_range(dt_holdout - timedelta(minutes=15 * NUM_REQ), dt_holdout + timedelta(minutes=15 * NUM_LABELS),\n",
    "                         freq=\"15min\")\n",
    "\n",
    "df_holdout['geohash6'] = geohash_holdout\n",
    "df_holdout = df_holdout.set_index('geohash6')\n",
    "dt_holdout_list = df_holdout_raw['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 0\n",
    "model_list = []\n",
    "y_pred, labels_list = [], [] #  val\n",
    "y_pred2, labels_list2 = [], [] # test\n",
    "\n",
    "dt_train = datetime(2019,5,23, 0, 0)\n",
    "dt_val = dt_train + timedelta(days=1)\n",
    "\n",
    "dt_holdout = datetime(2019, 5, 20) # arbitrary # Make sure same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiaWei\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "num_samples = 65 # to use: 400\n",
    "\n",
    "X_train, y_train = prep_dataset(dt_train, num_samples, df, cluster_df)\n",
    "X_val, y_val = prep_dataset(dt_val, 1, df, cluster_df)\n",
    "\n",
    "X_holdout, y_holdout = gen_test_features(dt_holdout, df_holdout, geohash_holdout, dt_holdout_list, cluster_df)\n",
    "X_holdout = X_holdout.reindex_axis(X_train.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: pickle fitted scaler for production\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_holdout]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_holdout[:] = scaler.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiaWei\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\lightgbm\\basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\JiaWei\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\lightgbm\\basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's rmse: 0.140225\tvalid_1's rmse: nan\n"
     ]
    }
   ],
   "source": [
    "MAX_ROUNDS = 200 # to use 2000\n",
    "params = {\n",
    "    'learning_rate' : 0.1,\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 800,\n",
    "    'min_split_gain': 0.03,\n",
    "     'subsample_for_bin': 3000,\n",
    "  'bagging_fraction': 0.5 # for speed, doesnt affect score\n",
    "}\n",
    "\n",
    "cate_vars = ['demand-3500', 'demand-3000', 'demand-2800', 'demand-2500', 'demand-2000',\n",
    "            'lat', 'lon', 'grid_id_by_lat', 'grid_id_by_lon']\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "print('Model {}'.format(round))\n",
    "\n",
    "dtrain = lgb.Dataset(\n",
    "            X_train, label=y_train,\n",
    "            categorical_feature=cate_vars)\n",
    "\n",
    "dval = lgb.Dataset(\n",
    "            X_holdout, label=y_holdout, reference=dtrain,\n",
    "            categorical_feature=cate_vars)\n",
    "\n",
    "model = lgb.train(\n",
    "                params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "                valid_sets=[dtrain, dval], early_stopping_rounds=20,\n",
    "                verbose_eval=50, evals_result=evals_result)\n",
    "    \n",
    "model_list.append(model)\n",
    "# to do: save model to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for T+%s 2\n",
      "Validation rmse: 0.08677808335449491\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-46a14202c2a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation rmse:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_rms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mholdout_rms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pred2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels_list2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Holdout rmse:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mholdout_rms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 241\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GrabTraffic\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Handle prediction\n",
    "\n",
    "print(\"Predictions for T+%s\", round+1)\n",
    "\n",
    "y_pred.append(model.predict(X_val).tolist())\n",
    "labels_list.append(y_val.tolist())\n",
    "\n",
    "y_pred2.append(model.predict(X_holdout).tolist())\n",
    "labels_list2.append(y_holdout.tolist())\n",
    "\n",
    "val_rms = math.sqrt(mean_squared_error([item for items in y_pred for item in items], [item for items in labels_list for item in items]))\n",
    "print(\"Validation rmse:\", val_rms)\n",
    "\n",
    "holdout_rms = math.sqrt(mean_squared_error([item for items in y_pred2 for item in items], [item for items in labels_list2 for item in items]))\n",
    "print(\"Holdout rmse:\", holdout_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling forecast\n",
    "# Rerun all cells starting from \"Generate train, val, test sets\" to get the next set of predictions\n",
    "# Repeat until T+5 to get the overall holdout rmse\n",
    "\n",
    "round += 1\n",
    "dt_train = dt_train + timedelta(minutes=15)\n",
    "dt_val = dt_train + timedelta(days=7)\n",
    "dt_holdout = dt_holdout + timedelta(minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: save predictions\n",
    "# save models here too if possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
